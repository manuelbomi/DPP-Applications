# DPP-Applications


# Overview of DPP

#### What is DDP (DistributedDataParallel)?

#### DDP = Distributed Data Parallel

##### It is the standard way to train large deep-learning models across multiple GPUs and multiple machines efficiently.

##### DDP is used in:

- PyTorch (torch.nn.parallel.DistributedDataParallel)

- TensorFlow MultiWorkerMirroredStrategy

- DeepSpeedâ€™s ZeRO stages

- Horovod

- Megatron-LM

- NCCL (backend)

- OpenAI / DeepSeek / Meta training infrastructure

*It is the backbone of modern LLM training.* 

---

## Why Do We Need DDP ?

#### As models grow:

- parameters exceed a single GPU

- datasets become massive

- training becomes too slow on one GPU

#### DDP solves this by replicating the same model on many GPUs, feeding each GPU a different slice of the data (â€œdata parallelismâ€), then synchronizing the gradients so they all stay identical.

---

## How DDP Works (Core Idea)

#### Let us say you have 4 GPUs to support your AI workload training

#### Step-by-step DDP Approach:

<ins>1. DDP Will Replicate the model</ins>

#### Each GPU gets an identical copy of the neural network parameters.

```python
GPU0: model copy
GPU1: model copy
GPU2: model copy
GPU3: model copy
```

<ins>2. DDP WillDistribute the batch</ins>

#### DDP splits the input batch across GPUs.

#### If batch_size = 128:

```python
GPU0 â†’ 32 samples  
GPU1 â†’ 32 samples  
GPU2 â†’ 32 samples  
GPU3 â†’ 32 samples
```

<ins>3. Each GPU computes forward + backward</ins>

#### Each GPU:

- runs the forward pass locally

- calculates synthetic gradients (your preferred term ğŸ˜Š)

#### gradients are local and differ based on that particular GPUâ€™s data shard

4. AllReduce: average gradients across all GPUs

This is the magic step.

If 4 GPUs have gradients:

```python
       Data Shards         Local Backprop       AllReduce            Sync Update
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU0 batch â”‚ â†’ fwd/bwd â†’ grad0 â†’ â”€â”       â”‚              â”‚      â”‚              â”‚
â”‚ GPU1 batch â”‚ â†’ fwd/bwd â†’ grad1 â†’ â”€â”¼â”€ Avg â†’â”‚ avg_grad ----â”¼â”€ Apply optimizer --> synced models
â”‚ GPU2 batch â”‚ â†’ fwd/bwd â†’ grad2 â†’ â”€â”¼       â”‚              â”‚
â”‚ GPU3 batch â”‚ â†’ fwd/bwd â†’ grad3 â†’ â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```
